{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurakn/undergrad_research_2022/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the parameters"
      ],
      "metadata": {
        "id": "2AS6CN95QY89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy file path\n",
        "TRAIN_DATASET_PATH = '/content/drive/MyDrive/Research/Fairness/code/dataset/adult_income/adult.data'\n",
        "TEST_DATASET_PATH = '/content/drive/MyDrive/Research/Fairness/code/dataset/adult_income/adult.test'\n",
        "NAME_DATASET_PATH = '/content/drive/MyDrive/Research/Fairness/code/dataset/adult_income/adult.names'\n",
        "\n",
        "# define the column name of the dataset, required\n",
        "column_name = [\"age\", \"workClass\", \"fnlwgt\", \"education\", \"education-num\",\"marital-status\", \"occupation\", \"relationship\",\n",
        "          \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"]\n",
        "\n",
        "\n",
        "drop_na = True"
      ],
      "metadata": {
        "id": "QswOuDCVQXdj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Nnrbzi6BQczJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rhWMcFl8KVaL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvSiFFTNQNE6",
        "outputId": "db9d59c1-9135-467b-c4ac-eedbc53170e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "Sv3YZ7UcLlJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
      ],
      "metadata": {
        "id": "a9k2XM_oASet"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import DataFrame\n",
        "def adult_feature_engineering(df:DataFrame):\n",
        "  '''\n",
        "  df['native-country']=np.where(df['native-country']==' United-States',1,0)\n",
        "  df['marital-status']=np.where(df['marital-status']==' Married',1,0)\n",
        "  df['workClass']=np.where(df['workClass']==' Private',1,0)\n",
        "  df['sex']=np.where(df['sex']==' Male',1,0)\n",
        "  df['race']=np.where(df['race']==' White',1,0)\n",
        "  '''\n",
        "  df = df.drop(columns=['fnlwgt'])\n",
        "\n",
        "\n",
        "  # income < 50k is 0, otherwise 1\n",
        "  df['sex']=np.where(df['sex']==' Male',1,0)\n",
        "  df['income']=np.where(np.logical_or(df['income']==' <=50K', df['income']==' <=50K.'),0,1)\n",
        "  obj_col = df.select_dtypes(include=['object'])\n",
        "  \n",
        "  # Not sure how to encode education, relationship, and occupation using one-hot\n",
        "\n",
        "  df = df.drop(columns=obj_col.columns)\n",
        "\n",
        "  #df = pd.get_dummies(df,obj_col.columns)\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "F71Jms4DM7Lo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f_load_data(TRAIN_DATASET_PATH:str,\n",
        "                TEST_DATASET_PATH:str,\n",
        "                drop_na:bool = True,\n",
        "                column_name:list = None,\n",
        "                ):\n",
        "  assert(column_name != None)\n",
        "  train_dataset = pd.read_csv(TRAIN_DATASET_PATH, index_col=False, header=None,\n",
        "                              names=column_name)\n",
        "  \n",
        "  test_dataset = pd.read_csv(TEST_DATASET_PATH, index_col=False, header=None,\n",
        "                             names=column_name, skiprows=1)\n",
        "  \n",
        "  \n",
        "\n",
        "  print(f'shape of train: {train_dataset.shape}')\n",
        "  print(f'shape of test: {test_dataset.shape}')\n",
        "  \n",
        "  if drop_na == True:\n",
        "    train_dataset = train_dataset.replace({' ?': np.nan}).dropna()\n",
        "    test_dataset = test_dataset.replace({' ?': np.nan}).dropna()\n",
        "\n",
        "  # need ignore_index to recalculate new index\n",
        "  combined_dataset = pd.concat([train_dataset,test_dataset], ignore_index=True)\n",
        "  \n",
        "  int_col = combined_dataset.select_dtypes(include=['int64'])\n",
        "  #print(int_col.columns)\n",
        "\n",
        "\n",
        "  for col in int_col:\n",
        "    max_element = combined_dataset[col].max()\n",
        "    min_element = combined_dataset[col].min()\n",
        "    dis = max_element - min_element\n",
        "    combined_dataset[col]=(combined_dataset[col]-min_element)/dis\n",
        "\n",
        "  #scalar = StandardScaler()\n",
        "  #tmp = scalar.fit_transform(combined_dataset[int_col.columns])\n",
        "  #combined_dataset.drop(int_col, axis=1, inplace=True)\n",
        "  #combined_dataset = np.hstack((combined_dataset.values, tmp))\n",
        "\n",
        "  \n",
        "  combined_dataset = adult_feature_engineering(combined_dataset)\n",
        "\n",
        "  #display(combined_dataset)\n",
        "  #combined_dataset.head()\n",
        "  #combined_dataset.info()\n",
        "\n",
        "  #train_dataset.info()\n",
        "  #test_dataset.info()\n",
        "  #train_dataset.head()\n",
        "  \n",
        "  #print(combined_dataset)\n",
        "  #combined_dataset.head()\n",
        "  index = train_dataset.shape[0]\n",
        "\n",
        "  train_dataset = combined_dataset.iloc[:index,:]\n",
        "  test_dataset = combined_dataset.iloc[index:,:]\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "train_dataset, test_dataset = f_load_data(TRAIN_DATASET_PATH, TEST_DATASET_PATH,  True, column_name)\n",
        "\n",
        "print(f'shape of train: {train_dataset.shape}')\n",
        "print(f'shape of test: {test_dataset.shape}')\n",
        "display(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "g61PRSMCLfmn",
        "outputId": "06b8c06a-e236-4829-852b-f8f4ae83cd22"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train: (32561, 15)\n",
            "shape of test: (16281, 15)\n",
            "shape of train: (30162, 103)\n",
            "shape of test: (15060, 103)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-60636767-8ef3-4f47-b78c-4436fd4619c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>education-num</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>income</th>\n",
              "      <th>workClass_ Federal-gov</th>\n",
              "      <th>workClass_ Local-gov</th>\n",
              "      <th>workClass_ Private</th>\n",
              "      <th>workClass_ Self-emp-inc</th>\n",
              "      <th>workClass_ Self-emp-not-inc</th>\n",
              "      <th>workClass_ State-gov</th>\n",
              "      <th>workClass_ Without-pay</th>\n",
              "      <th>education_ 10th</th>\n",
              "      <th>education_ 11th</th>\n",
              "      <th>education_ 12th</th>\n",
              "      <th>education_ 1st-4th</th>\n",
              "      <th>education_ 5th-6th</th>\n",
              "      <th>education_ 7th-8th</th>\n",
              "      <th>education_ 9th</th>\n",
              "      <th>education_ Assoc-acdm</th>\n",
              "      <th>education_ Assoc-voc</th>\n",
              "      <th>education_ Bachelors</th>\n",
              "      <th>education_ Doctorate</th>\n",
              "      <th>education_ HS-grad</th>\n",
              "      <th>education_ Masters</th>\n",
              "      <th>education_ Preschool</th>\n",
              "      <th>education_ Prof-school</th>\n",
              "      <th>education_ Some-college</th>\n",
              "      <th>marital-status_ Divorced</th>\n",
              "      <th>marital-status_ Married-AF-spouse</th>\n",
              "      <th>marital-status_ Married-civ-spouse</th>\n",
              "      <th>marital-status_ Married-spouse-absent</th>\n",
              "      <th>marital-status_ Never-married</th>\n",
              "      <th>marital-status_ Separated</th>\n",
              "      <th>marital-status_ Widowed</th>\n",
              "      <th>occupation_ Adm-clerical</th>\n",
              "      <th>occupation_ Armed-Forces</th>\n",
              "      <th>occupation_ Craft-repair</th>\n",
              "      <th>...</th>\n",
              "      <th>native-country_ Canada</th>\n",
              "      <th>native-country_ China</th>\n",
              "      <th>native-country_ Columbia</th>\n",
              "      <th>native-country_ Cuba</th>\n",
              "      <th>native-country_ Dominican-Republic</th>\n",
              "      <th>native-country_ Ecuador</th>\n",
              "      <th>native-country_ El-Salvador</th>\n",
              "      <th>native-country_ England</th>\n",
              "      <th>native-country_ France</th>\n",
              "      <th>native-country_ Germany</th>\n",
              "      <th>native-country_ Greece</th>\n",
              "      <th>native-country_ Guatemala</th>\n",
              "      <th>native-country_ Haiti</th>\n",
              "      <th>native-country_ Holand-Netherlands</th>\n",
              "      <th>native-country_ Honduras</th>\n",
              "      <th>native-country_ Hong</th>\n",
              "      <th>native-country_ Hungary</th>\n",
              "      <th>native-country_ India</th>\n",
              "      <th>native-country_ Iran</th>\n",
              "      <th>native-country_ Ireland</th>\n",
              "      <th>native-country_ Italy</th>\n",
              "      <th>native-country_ Jamaica</th>\n",
              "      <th>native-country_ Japan</th>\n",
              "      <th>native-country_ Laos</th>\n",
              "      <th>native-country_ Mexico</th>\n",
              "      <th>native-country_ Nicaragua</th>\n",
              "      <th>native-country_ Outlying-US(Guam-USVI-etc)</th>\n",
              "      <th>native-country_ Peru</th>\n",
              "      <th>native-country_ Philippines</th>\n",
              "      <th>native-country_ Poland</th>\n",
              "      <th>native-country_ Portugal</th>\n",
              "      <th>native-country_ Puerto-Rico</th>\n",
              "      <th>native-country_ Scotland</th>\n",
              "      <th>native-country_ South</th>\n",
              "      <th>native-country_ Taiwan</th>\n",
              "      <th>native-country_ Thailand</th>\n",
              "      <th>native-country_ Trinadad&amp;Tobago</th>\n",
              "      <th>native-country_ United-States</th>\n",
              "      <th>native-country_ Vietnam</th>\n",
              "      <th>native-country_ Yugoslavia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30162</th>\n",
              "      <td>0.109589</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30163</th>\n",
              "      <td>0.287671</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30164</th>\n",
              "      <td>0.150685</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30165</th>\n",
              "      <td>0.369863</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.076881</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30166</th>\n",
              "      <td>0.232877</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.295918</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45217</th>\n",
              "      <td>0.219178</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45218</th>\n",
              "      <td>0.301370</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45219</th>\n",
              "      <td>0.287671</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45220</th>\n",
              "      <td>0.369863</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.054551</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45221</th>\n",
              "      <td>0.246575</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.602041</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15060 rows × 103 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60636767-8ef3-4f47-b78c-4436fd4619c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60636767-8ef3-4f47-b78c-4436fd4619c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60636767-8ef3-4f47-b78c-4436fd4619c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            age  ...  native-country_ Yugoslavia\n",
              "30162  0.109589  ...                           0\n",
              "30163  0.287671  ...                           0\n",
              "30164  0.150685  ...                           0\n",
              "30165  0.369863  ...                           0\n",
              "30166  0.232877  ...                           0\n",
              "...         ...  ...                         ...\n",
              "45217  0.219178  ...                           0\n",
              "45218  0.301370  ...                           0\n",
              "45219  0.287671  ...                           0\n",
              "45220  0.369863  ...                           0\n",
              "45221  0.246575  ...                           0\n",
              "\n",
              "[15060 rows x 103 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.index.is_unique"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl5TSD6BUOl9",
        "outputId": "d806cfe5-64f3-4af6-9d64-90c6dbc4d32e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Template"
      ],
      "metadata": {
        "id": "x6isP1haSRs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Template model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn\n",
        "\n",
        "class Abernethy_p_sampling(object):\n",
        "  def __init__(\n",
        "      self,\n",
        "      classifier,\n",
        "      p:float,\n",
        "      initial_set_frac:float = 0.1,\n",
        "      origin_train_dataset:DataFrame = None,\n",
        "      origin_test_dataset:DataFrame = None,\n",
        "      protected_label:str = None,\n",
        "      ):\n",
        "    assert(protected_label != None)\n",
        "    assert(0 < p <=1)\n",
        "    assert(0 < initial_set_frac <=1)\n",
        "    self.protected_label = protected_label\n",
        "    self.p = p\n",
        "    self.origin_train_dataset = origin_train_dataset.copy()\n",
        "    self.origin_test_dataset = origin_test_dataset.copy()\n",
        "    self.best_classifier = sklearn.base.clone(classifier)\n",
        "    self.empty_classifier = sklearn.base.clone(classifier)\n",
        "\n",
        "    # construct the initial training set and remove them from the original set\n",
        "    self.trainset = self.origin_train_dataset.sample(frac = initial_set_frac)\n",
        "    self.origin_train_dataset.drop(index=self.trainset.index, inplace=True)\n",
        "    \n",
        "    # Need to reset index to keep everything in order\n",
        "    self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "    self.trainset.reset_index(drop=True, inplace=True)\n",
        "    #display(self.trainset)\n",
        "    #display(self.origin_train_dataset)\n",
        "  \n",
        "  def train(\n",
        "      self,\n",
        "      chosen_metric:str = 'Overall_accuracy',\n",
        "      quiet_mode:bool = True,\n",
        "      stop_iter_num:int = 1000,\n",
        "      ):\n",
        "    self.scores_list = []\n",
        "    self.metric_result_list = []\n",
        "    self.run_out_time = {}\n",
        "    best_score = -1\n",
        "\n",
        "    unique_p_label = self.origin_train_dataset[self.protected_label].unique()\n",
        "    for i in unique_p_label:\n",
        "      self.run_out_time[i] = -1\n",
        "\n",
        "    stop_training = False\n",
        "    train_data = self.trainset.copy()\n",
        "    y = train_data['income']\n",
        "    X = train_data.drop('income', axis=1)\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y)\n",
        "\n",
        "    # fit the initial model and output the prediction\n",
        "    c_classifier = sklearn.base.clone(self.empty_classifier)\n",
        "    c_classifier.fit(X_train, y_train)\n",
        "    y_pred = c_classifier.predict(X_test)\n",
        "\n",
        "    # obtain score and fairness violation\n",
        "    initial_score = c_classifier.score(X_test, y_test)\n",
        "    best_score = initial_score\n",
        "    self.scores_list.append(initial_score)\n",
        "    metric = Fairness_metric(X_test[self.protected_label], true_label=y_test, predict_label=y_pred)\n",
        "    metric.process()\n",
        "    all_violation = metric.calculate_all()\n",
        "    self.metric_result_list.append(all_violation)\n",
        "    c_label = all_violation[chosen_metric][1]\n",
        "    c_vio = all_violation[chosen_metric][2]\n",
        "    if quiet_mode == False: \n",
        "      print(f'Initial Score: {initial_score}, Initial violation: {c_vio}, Initial biased group: {c_label}')\n",
        "    \n",
        "    #display(train_data)\n",
        "    # sample from biased group\n",
        "    t_random_num = np.random.uniform()\n",
        "    if t_random_num < self.p:\n",
        "       smp = self.origin_train_dataset[self.origin_train_dataset[self.protected_label] == c_label].sample()\n",
        "       self.origin_train_dataset.drop(index=smp.index, inplace=True)\n",
        "       self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "       train_data = pd.concat([train_data, smp], ignore_index=True)\n",
        "    else:\n",
        "      smp = self.origin_train_dataset.sample()\n",
        "      self.origin_train_dataset.drop(index=smp.index, inplace=True)\n",
        "      self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "      train_data = pd.concat([train_data, smp], ignore_index=True)\n",
        "\n",
        "    iteration = 1\n",
        "\n",
        "    \n",
        "\n",
        "    while stop_training == False:\n",
        "      y = train_data['income']\n",
        "      X = train_data.drop('income', axis=1)\n",
        "      X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y)\n",
        "\n",
        "\n",
        "      self.X_train_backup, self.y_train_backup = X_train, y_train\n",
        "\n",
        "      # fit the initial model and output the prediction\n",
        "      c_classifier = sklearn.base.clone(self.empty_classifier)\n",
        "      \n",
        "      c_classifier.fit(X_train, y_train)\n",
        "      y_pred = c_classifier.predict(X_test)\n",
        "\n",
        "      # obtain score and fairness violation\n",
        "      c_score = c_classifier.score(X_test, y_test)\n",
        "      self.scores_list.append(c_score)\n",
        "      metric = Fairness_metric(X_test[self.protected_label], true_label=y_test, predict_label=y_pred)\n",
        "      #print(X_test[self.protected_label])\n",
        "      \n",
        "      metric.process()\n",
        "      all_violation = metric.calculate_all()\n",
        "      self.metric_result_list.append(all_violation)\n",
        "      c_label = all_violation[chosen_metric][1]\n",
        "      c_vio = all_violation[chosen_metric][2]\n",
        "      if quiet_mode == False and iteration % 500 == 0: \n",
        "        print(f'Score: {c_score}, violation: {c_vio}, biased group: {c_label}')\n",
        "\n",
        "      #print(all_violation)\n",
        "\n",
        "      \n",
        "      if c_score > best_score:\n",
        "        best_score = c_score\n",
        "        self.best_classifier = c_classifier\n",
        "\n",
        "      # sample from biased group\n",
        "      t_random_num = np.random.uniform()\n",
        "      biased_set = self.origin_train_dataset[self.origin_train_dataset[self.protected_label] == c_label]\n",
        "      if len(biased_set) == 0 and self.run_out_time[c_label] == -1:\n",
        "        print(f'Run out of sample for group {c_label} at iteration {iteration}')\n",
        "        self.run_out_time[c_label] = iteration\n",
        "\n",
        "      if t_random_num < self.p and len(biased_set) != 0:\n",
        "        smp = self.origin_train_dataset[self.origin_train_dataset[self.protected_label] == c_label].sample()\n",
        "        \n",
        "        self.origin_train_dataset.drop(index=smp.index, inplace=True)\n",
        "        self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "        train_data = pd.concat([train_data, smp], ignore_index=True)\n",
        "      else:\n",
        "        smp = self.origin_train_dataset.sample()\n",
        "        self.origin_train_dataset.drop(index=smp.index, inplace=True)\n",
        "        self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "        train_data = pd.concat([train_data, smp], ignore_index=True)\n",
        "\n",
        "      if len(self.origin_train_dataset) == 0 or iteration >= stop_iter_num:\n",
        "        stop_training = True\n",
        "      iteration += 1\n",
        "\n",
        "\n",
        "    # How to define the best classifier?\n",
        "    return self.best_classifier, self.scores_list, self.metric_result_list\n",
        "  \n",
        "  def predict(\n",
        "      self,\n",
        "      ):\n",
        "    return None\n",
        "  \n",
        "  def save(\n",
        "      self,\n",
        "      path:str):\n",
        "    return None\n",
        "  \n",
        "  def load(\n",
        "      self,\n",
        "      path:str):\n",
        "    return None"
      ],
      "metadata": {
        "id": "XDRdZD3lYLt8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = LogisticRegression( max_iter = 500)\n",
        "model = Abernethy_p_sampling(classifier, 0.7, 0.1, train_dataset, test_dataset, 'sex')\n",
        "len(train_dataset)\n",
        "best_classifier, scores_list, metric_result_list = model.train(quiet_mode = False, chosen_metric='Demographic_Parity', stop_iter_num = 3000)\n",
        "# The number of rows do not add up"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vxahce5WMA9",
        "outputId": "b60f6542-0e8a-4a24-f387-0a5bfb80ac17"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Score: 0.8328912466843501, Initial violation: [-0.16188185], Initial biased group: 0\n",
            "Score: 0.8339021615472128, violation: [-0.21505891], biased group: 0\n",
            "Score: 0.8456175298804781, violation: [-0.16288228], biased group: 0\n",
            "Score: 0.8396811337466785, violation: [-0.19837114], biased group: 0\n",
            "Score: 0.8628389154704944, violation: [-0.18704716], biased group: 0\n",
            "Score: 0.8520667150108775, violation: [-0.18512485], biased group: 0\n",
            "Score: 0.8497340425531915, violation: [-0.17347652], biased group: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vio_list = []\n",
        "for dict_i in metric_result_list:\n",
        "  vio_list.append(abs(dict_i['Equal_Opportunity'][2]))\n",
        "plt.plot(vio_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "KEHTk8w67tmV",
        "outputId": "b972bcf2-c7b2-42f0-a97d-c5ac3f27f6ab"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fde150cc850>]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "smp = train_dataset[train_dataset[\"sex\"]== 0].sample()\n",
        "print(smp)\n",
        "index = smp.index\n",
        "print(train_dataset.iloc[index])\n",
        "#print(smp.index)\n",
        "train_dataset.drop(index=smp.index, inplace=True)\n",
        "train_dataset.reset_index(drop=True, inplace=True)\n",
        "print(train_dataset.iloc[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqvR4wiNNAq0",
        "outputId": "b6517d79-a70f-4fc2-a7e1-ae3a514a772d"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            age  ...  native-country_ Yugoslavia\n",
            "12167  0.639442  ...                           0\n",
            "\n",
            "[1 rows x 103 columns]\n",
            "            age  ...  native-country_ Yugoslavia\n",
            "12167  0.639442  ...                           0\n",
            "\n",
            "[1 rows x 103 columns]\n",
            "            age  ...  native-country_ Yugoslavia\n",
            "12167  0.715097  ...                           0\n",
            "\n",
            "[1 rows x 103 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.index.is_unique"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSyc5V-qRwEY",
        "outputId": "91d05dfb-47a8-4789-e52d-141b7eaef789"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shekhar"
      ],
      "metadata": {
        "id": "VTzrG5kgybHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Template model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn\n",
        "\n",
        "class Abernethy_p_sampling(object):\n",
        "  def __init__(\n",
        "      self,\n",
        "      classifier,\n",
        "      p:float,\n",
        "      initial_set_frac:float = 0.3,\n",
        "      origin_train_dataset:DataFrame = None,\n",
        "      origin_test_dataset:DataFrame = None,\n",
        "      protected_label:str = None,\n",
        "      ):\n",
        "    assert(protected_label != None)\n",
        "    assert(0 < p <=1)\n",
        "    assert(0 < initial_set_frac <=1)\n",
        "    self.protected_label = protected_label\n",
        "    self.p = p\n",
        "    self.origin_train_dataset = origin_train_dataset.copy()\n",
        "    self.origin_test_dataset = origin_test_dataset.copy()\n",
        "    self.best_classifier = sklearn.base.clone(classifier)\n",
        "    self.empty_classifier = sklearn.base.clone(classifier)\n",
        "\n",
        "    # construct the initial training set and remove them from the original set\n",
        "    self.trainset = self.origin_train_dataset.sample(frac = initial_set_frac)\n",
        "    self.origin_train_dataset.drop(index=self.trainset.index, inplace=True)\n",
        "    \n",
        "    # Need to reset index to keep everything in order\n",
        "    self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "    self.trainset.reset_index(drop=True, inplace=True)\n",
        "    #display(self.trainset)\n",
        "    #display(self.origin_train_dataset)\n",
        "  \n",
        "  def train(\n",
        "      self,\n",
        "      chosen_metric:str = 'Overall_accuracy',\n",
        "      quiet_mode:bool = True\n",
        "      ):\n",
        "    self.scores_list = []\n",
        "    self.metric_result_list = []\n",
        "    self.run_out_time = {}\n",
        "    best_score = -1\n",
        "\n",
        "    unique_p_label = self.origin_train_dataset[self.protected_label].unique()\n",
        "    for i in unique_p_label:\n",
        "      self.run_out_time[i] = -1\n",
        "\n",
        "    stop_training = False\n",
        "    train_data = self.trainset.copy()\n",
        "    y = train_data['income']\n",
        "    X = train_data.drop('income', axis=1)\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y)\n",
        "\n",
        "    # fit the initial model and output the prediction\n",
        "    c_classifier = sklearn.base.clone(self.empty_classifier)\n",
        "    c_classifier.fit(X_train, y_train)\n",
        "    y_pred = c_classifier.predict(X_test)\n",
        "\n",
        "    # obtain score and fairness violation\n",
        "    initial_score = c_classifier.score(X_test, y_test)\n",
        "    best_score = initial_score\n",
        "    self.scores_list.append(initial_score)\n",
        "    metric = Fairness_metric(X_test[self.protected_label], true_label=y_test, predict_label=y_pred)\n",
        "    metric.process()\n",
        "    all_violation = metric.calculate_all()\n",
        "    self.metric_result_list.append(all_violation)\n",
        "    c_label = all_violation[chosen_metric][1]\n",
        "    c_vio = all_violation[chosen_metric][2]\n",
        "    if quiet_mode == False: \n",
        "      print(f'Initial Score: {initial_score}, Initial violation: {c_vio}, Initial biased group: {c_label}')\n",
        "    \n",
        "    #display(train_data)\n",
        "    # sample from biased group\n",
        "    t_random_num = np.random.uniform()\n",
        "    if t_random_num < self.p:\n",
        "       smp = self.origin_train_dataset[self.origin_train_dataset[self.protected_label] == c_label].sample()\n",
        "       self.origin_train_dataset.drop(index=smp.index, inplace=True)\n",
        "       self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "       train_data = pd.concat([train_data, smp], ignore_index=True)\n",
        "    else:\n",
        "      smp = self.origin_train_dataset.sample()\n",
        "      self.origin_train_dataset.drop(index=smp.index, inplace=True)\n",
        "      self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "      train_data = pd.concat([train_data, smp], ignore_index=True)\n",
        "\n",
        "    iteration = 1\n",
        "    while stop_training == False:\n",
        "      y = train_data['income']\n",
        "      X = train_data.drop('income', axis=1)\n",
        "      X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y)\n",
        "\n",
        "      # fit the initial model and output the prediction\n",
        "      c_classifier = sklearn.base.clone(self.empty_classifier)\n",
        "      \n",
        "      c_classifier.fit(X_train, y_train)\n",
        "      y_pred = c_classifier.predict(X_test)\n",
        "\n",
        "      # obtain score and fairness violation\n",
        "      c_score = c_classifier.score(X_test, y_test)\n",
        "      self.scores_list.append(c_score)\n",
        "      metric = Fairness_metric(X_test[self.protected_label], true_label=y_test, predict_label=y_pred)\n",
        "      #print(X_test[self.protected_label])\n",
        "      \n",
        "      metric.process()\n",
        "      all_violation = metric.calculate_all()\n",
        "      self.metric_result_list.append(all_violation)\n",
        "      c_label = all_violation[chosen_metric][1]\n",
        "      c_vio = all_violation[chosen_metric][2]\n",
        "      if quiet_mode == False and iteration % 500 == 0: \n",
        "        print(f'Score: {c_score}, violation: {c_vio}, biased group: {c_label}')\n",
        "\n",
        "      #print(all_violation)\n",
        "\n",
        "      \n",
        "      if c_score > best_score:\n",
        "        best_score = c_score\n",
        "        self.best_classifier = c_classifier\n",
        "\n",
        "      # sample from biased group\n",
        "      t_random_num = np.random.uniform()\n",
        "      biased_set = self.origin_train_dataset[self.origin_train_dataset[self.protected_label] == c_label]\n",
        "      if len(biased_set) == 0 and self.run_out_time[c_label] == -1:\n",
        "        print(f'Run out of sample for group {c_label} at iteration {iteration}')\n",
        "        self.run_out_time[c_label] = iteration\n",
        "\n",
        "      if t_random_num < self.p and len(biased_set) != 0:\n",
        "        smp = self.origin_train_dataset[self.origin_train_dataset[self.protected_label] == c_label].sample()\n",
        "        \n",
        "        self.origin_train_dataset.drop(index=smp.index, inplace=True)\n",
        "        self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "        train_data = pd.concat([train_data, smp], ignore_index=True)\n",
        "      else:\n",
        "        smp = self.origin_train_dataset.sample()\n",
        "        self.origin_train_dataset.drop(index=smp.index, inplace=True)\n",
        "        self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "        train_data = pd.concat([train_data, smp], ignore_index=True)\n",
        "\n",
        "      if len(self.origin_train_dataset) == 0:\n",
        "        stop_training = True\n",
        "      iteration += 1\n",
        "\n",
        "\n",
        "    # How to define the best classifier?\n",
        "    return self.best_classifier, self.scores_list, self.metric_result_list\n",
        "  \n",
        "  def predict(\n",
        "      self,\n",
        "      ):\n",
        "    return None\n",
        "  \n",
        "  def save(\n",
        "      self,\n",
        "      path:str):\n",
        "    return None\n",
        "  \n",
        "  def load(\n",
        "      self,\n",
        "      path:str):\n",
        "    return None"
      ],
      "metadata": {
        "id": "jHW0vyPLyWEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fairness Metric"
      ],
      "metadata": {
        "id": "dJws2nDZSZkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Xlte9p0KScjZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#self.true_data = train_dataset[[protected_col,label_col]].to_numpy()\n",
        "# true_data is |protected_label | true_y |\n",
        "\n",
        "class Fairness_metric(object):\n",
        "  def __init__(\n",
        "      self,\n",
        "      protected_label:list,\n",
        "      true_label:list,\n",
        "      predict_label:list,\n",
        "      ):\n",
        "    assert(len(protected_label) == len(true_label))\n",
        "    assert(len(predict_label) == len(true_label))\n",
        "    self.protected_label = np.array(protected_label)\n",
        "    self.predict_label = np.array(predict_label)\n",
        "    self.true_label = np.array(true_label)\n",
        "    self.res_mat = {}\n",
        "    # res_mat: \n",
        "    # key : protected label \n",
        "    # val: [TP, FP, FN, TN]\n",
        "    \n",
        "  def process(self):\n",
        "    for key in np.unique(self.protected_label):\n",
        "      self.res_mat[key] = [0,0,0,0]\n",
        "      # TP, FP, FN, TN\n",
        "    for i in range(len(self.predict_label)):\n",
        "      tmp_prot = self.protected_label[i]\n",
        "      tmp_true = self.true_label[i]\n",
        "      tmp_pred = self.predict_label[i]\n",
        "      if tmp_true == tmp_pred and tmp_true == 1:\n",
        "        # TP\n",
        "        self.res_mat[tmp_prot][0] +=1\n",
        "      elif tmp_true == 1 and tmp_pred == 0:\n",
        "        # FN\n",
        "        self.res_mat[tmp_prot][2] +=1\n",
        "      elif tmp_true == 0 and tmp_pred == 1:\n",
        "        # FP\n",
        "        self.res_mat[tmp_prot][1] +=1\n",
        "      elif tmp_true == 0 and tmp_pred == 0:\n",
        "        # TN\n",
        "        self.res_mat[tmp_prot][3] +=1\n",
        "      else:\n",
        "        print('Incorrect Code')\n",
        "\n",
        "  # This is for two protected labels, can be adapted to multiple protected labels\n",
        "  # output an dict of scores of all labels and a list containing biased group with\n",
        "  # its violation\n",
        "  def Demographic_Parity(\n",
        "      self,\n",
        "      ):\n",
        "    res = {}\n",
        "    # [label : score]\n",
        "    for label, tmp_list in self.res_mat.items():\n",
        "      TP = tmp_list[0]\n",
        "      FP = tmp_list[1]\n",
        "      FN = tmp_list[2]\n",
        "      TN = tmp_list[3]\n",
        "      tot = np.sum(tmp_list)\n",
        "      res[label] = [TP+FP]/tot\n",
        "\n",
        "    min_violation = 99\n",
        "    min_label = None\n",
        "\n",
        "    for label, score in res.items():\n",
        "      for label_2, score_2 in res.items():\n",
        "        if label == label_2:\n",
        "          pass\n",
        "        vio = score-score_2\n",
        "        if vio < min_violation:\n",
        "          min_violation = vio\n",
        "          min_label = label\n",
        "    \n",
        "    assert(min_violation != 99)\n",
        "    assert(-1 <= min_violation <= 0)\n",
        "    assert(min_label != None)\n",
        "    return res , min_label, min_violation\n",
        "  \n",
        "  def Equal_Opportunity(\n",
        "      self,\n",
        "      ):\n",
        "    res = {}\n",
        "    # [label : score]\n",
        "    for label, tmp_list in self.res_mat.items():\n",
        "      \n",
        "      TP = tmp_list[0]\n",
        "      FP = tmp_list[1]\n",
        "      FN = tmp_list[2]\n",
        "      TN = tmp_list[3]\n",
        "      tot = np.sum(tmp_list)\n",
        "      res[label] =[TP]/tot\n",
        "\n",
        "    min_violation = 99\n",
        "    min_label = None\n",
        "\n",
        "    for label, score in res.items():\n",
        "      for label_2, score_2 in res.items():\n",
        "        if label == label_2:\n",
        "          pass\n",
        "        vio = score-score_2\n",
        "        if vio < min_violation:\n",
        "          min_violation = vio\n",
        "          min_label = label\n",
        "    \n",
        "    assert(min_violation != 99)\n",
        "    assert(-1 <= min_violation <= 0)\n",
        "    assert(min_label != None)\n",
        "    return res , min_label, min_violation\n",
        "  \n",
        "  def Equal_Odds(\n",
        "      self,\n",
        "      ):\n",
        "    res = {}\n",
        "    # [label : score]\n",
        "    for label, tmp_list in self.res_mat.items():\n",
        "      TP = tmp_list[0]\n",
        "      FP = tmp_list[1]\n",
        "      FN = tmp_list[2]\n",
        "      TN = tmp_list[3]\n",
        "      tot = np.sum(tmp_list)\n",
        "      res[label] =[TP+FP]/tot\n",
        "\n",
        "    min_violation = 99\n",
        "    min_label = None\n",
        "\n",
        "    for label, score in res.items():\n",
        "      for label_2, score_2 in res.items():\n",
        "        if label == label_2:\n",
        "          pass\n",
        "        vio = score-score_2\n",
        "        if vio < min_violation:\n",
        "          min_violation = vio\n",
        "          min_label = label\n",
        "    \n",
        "    assert(min_violation != 99)\n",
        "    assert(-1 <= min_violation <= 0)\n",
        "    assert(min_label != None)\n",
        "    return res , min_label, min_violation\n",
        "\n",
        "  def Overall_accuracy(\n",
        "      self,\n",
        "      ):\n",
        "    res = {}\n",
        "    # [label : score]\n",
        "    for label, tmp_list in self.res_mat.items():\n",
        "      TP = tmp_list[0]\n",
        "      FP = tmp_list[1]\n",
        "      FN = tmp_list[2]\n",
        "      TN = tmp_list[3]\n",
        "      tot = np.sum(tmp_list)\n",
        "      res[label] =[TP+FN]/tot\n",
        "\n",
        "    min_violation = 99\n",
        "    min_label = None\n",
        "\n",
        "    for label, score in res.items():\n",
        "      for label_2, score_2 in res.items():\n",
        "        if label == label_2:\n",
        "          pass\n",
        "        vio = score-score_2\n",
        "        if vio < min_violation:\n",
        "          min_violation = vio\n",
        "          min_label = label\n",
        "    \n",
        "    assert(min_violation != 99)\n",
        "    assert(-1 <= min_violation <= 0)\n",
        "    assert(min_label != None)\n",
        "    return res , min_label, min_violation\n",
        "\n",
        "\n",
        "  # return dict with {metric_name: (score, highest violation label, violation)}\n",
        "  def calculate_all(self):\n",
        "    return_dict = {}\n",
        "    return_dict['Demographic_Parity'] = self.Demographic_Parity()\n",
        "    return_dict['Equal_Opportunity'] = self.Equal_Opportunity()\n",
        "    return_dict['Equal_Odds'] = self.Equal_Odds()\n",
        "    return_dict['Overall_accuracy'] = self.Overall_accuracy()\n",
        "    return return_dict\n"
      ],
      "metadata": {
        "id": "pmNlFx3OTsrB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "protected_label = [0, 1, 0, 1 , 1]\n",
        "predict_label = [1,0,1,1,0]\n",
        "true_label = [1, 1, 0, 1, 0 ]\n",
        "\n",
        "metric = Fairness_metric(protected_label, true_label, predict_label)\n",
        "metric.process()\n",
        "#print(metric.Overall_accuracy())\n",
        "all_data = metric.calculate_all()\n",
        "c_label = all_data['Overall_accuracy'][2]\n",
        "print(all_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjsSi8mJnRjA",
        "outputId": "0f4b54d9-018f-4d94-8199-841955482c42"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Demographic_Parity': ({0: array([1.]), 1: array([0.33333333])}, 0, array([0.66666667])), 'Equal_Opportunity': ({0: array([0.5]), 1: array([0.33333333])}, 0, array([0.16666667])), 'Equal_Odds': ({0: array([1.]), 1: array([0.33333333])}, 0, array([0.66666667])), 'Overall_accuracy': ({0: array([0.5]), 1: array([0.66666667])}, 1, array([0.16666667]))}\n"
          ]
        }
      ]
    }
  ]
}