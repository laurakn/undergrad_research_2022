{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurakn/undergrad_research_2022/blob/master/Updated_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60SyTEjwwpWL"
      },
      "source": [
        "Turns out my computer rejects the tab spacing you're using, so I had to adjust the spacing in the parts of the code I wanted to run, which will probably make it annoying for you to if you open this. If you're writing this in Colab, maybe try changing the tab from 2 spaces to 4 (pretty sure there is a simple way to do this)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AS6CN95QY89"
      },
      "source": [
        "# Define the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QswOuDCVQXdj"
      },
      "outputs": [],
      "source": [
        "# copy file path\n",
        "TRAIN_DATASET_PATH = '/content/drive/MyDrive/Research/Fairness/code/dataset/adult_income/adult.data'\n",
        "TEST_DATASET_PATH = '/content/drive/MyDrive/Research/Fairness/code/dataset/adult_income/adult.test'\n",
        "NAME_DATASET_PATH = '/content/drive/MyDrive/Research/Fairness/code/dataset/adult_income/adult.names'\n",
        "\n",
        "RESULT_SAVE_PATH = '/content/drive/MyDrive/Research/Fairness/result'\n",
        "\n",
        "\n",
        "# define the column name of the dataset, required\n",
        "column_name = [\"age\", \"workClass\", \"fnlwgt\", \"education\", \"education-num\",\"marital-status\", \"occupation\", \"relationship\",\n",
        "          \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"]\n",
        "\n",
        "\n",
        "drop_na = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnrbzi6BQczJ"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YdSkQ-UL0lw",
        "outputId": "9d294f35-25de-4136-af80-b5a506d19f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhWMcFl8KVaL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder as ohe\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv3YZ7UcLlJI"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9k2XM_oASet"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTYbQTiwwpWU"
      },
      "source": [
        "Why are you dropping 'fnlwgt'? \n",
        "\n",
        "You don't have to load DataFrame, could use instead pd.DataFame.\n",
        "\n",
        "Would recommend using map instead of where (as added below). Also make sure to have features called 'group' and 'label'. Then if we want to run everything with a different dataset, we don't need to change any code, just two column names.\n",
        "\n",
        "Also, have to be careful with the label, since I think one file includes a period after ' >50K' and one doesn't"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F71Jms4DM7Lo"
      },
      "outputs": [],
      "source": [
        "from pandas.core.frame import DataFrame\n",
        "def adult_feature_engineering(df:DataFrame):\n",
        "#     df = df.drop(columns=['fnlwgt']) \n",
        "\n",
        "\n",
        "#     # income < 50k is 0, otherwise 1\n",
        "#     df['sex']=np.where(df['sex']==' Male',1,0)\n",
        "#     df['income']=np.where(np.logical_or(df['income']==' <=50K', df['income']==' <=50K.'),0,1)\n",
        "#     obj_df = df.select_dtypes(include=['object'])\n",
        "    \n",
        "    # map is a little easier to understand, then it is explicitly clear which variable corresponds to 0, and 1\n",
        "    df['group'] = df['sex'].map({' Male':1, ' Female':0})\n",
        "    df['label'] = df['income'].map({' <=50K.':0, ' <=50K':0, ' >50K.':1, ' >50K':1})\n",
        "\n",
        "    df = df.drop(['sex', 'income'],axis=1)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMH6C3T6wpWW"
      },
      "source": [
        "Can you add comments to the f_load_data? It's not clear from the naming that it is doing more than just loading data. \n",
        "\n",
        "Should be clear what data cleaning is happening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g61PRSMCLfmn",
        "outputId": "ea7878f6-3dc2-4076-a390-2022c82f249e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train: (32561, 15)\n",
            "shape of test: (16281, 15)\n",
            "shape of train: (30162, 104)\n",
            "shape of test: (15060, 104)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8ce6b593-87da-44f0-831e-3751b7a3010c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education-num</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>group</th>\n",
              "      <th>label</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>...</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30162</th>\n",
              "      <td>0.109589</td>\n",
              "      <td>0.144430</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30163</th>\n",
              "      <td>0.287671</td>\n",
              "      <td>0.051677</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30164</th>\n",
              "      <td>0.150685</td>\n",
              "      <td>0.219011</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30165</th>\n",
              "      <td>0.369863</td>\n",
              "      <td>0.099418</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.076881</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.397959</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30166</th>\n",
              "      <td>0.232877</td>\n",
              "      <td>0.125398</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.295918</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 104 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ce6b593-87da-44f0-831e-3751b7a3010c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8ce6b593-87da-44f0-831e-3751b7a3010c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8ce6b593-87da-44f0-831e-3751b7a3010c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
              "30162  0.109589  0.144430       0.400000      0.000000           0.0   \n",
              "30163  0.287671  0.051677       0.533333      0.000000           0.0   \n",
              "30164  0.150685  0.219011       0.733333      0.000000           0.0   \n",
              "30165  0.369863  0.099418       0.600000      0.076881           0.0   \n",
              "30166  0.232877  0.125398       0.333333      0.000000           0.0   \n",
              "\n",
              "       hours-per-week  group  label    0    1  ...   86   87   88   89   90  \\\n",
              "30162        0.397959    1.0    0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0   \n",
              "30163        0.500000    1.0    0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0   \n",
              "30164        0.397959    1.0    1.0  0.0  1.0  ...  0.0  0.0  0.0  0.0  0.0   \n",
              "30165        0.397959    1.0    1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0   \n",
              "30166        0.295918    1.0    0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0   \n",
              "\n",
              "        91   92   93   94   95  \n",
              "30162  0.0  0.0  1.0  0.0  0.0  \n",
              "30163  0.0  0.0  1.0  0.0  0.0  \n",
              "30164  0.0  0.0  1.0  0.0  0.0  \n",
              "30165  0.0  0.0  1.0  0.0  0.0  \n",
              "30166  0.0  0.0  1.0  0.0  0.0  \n",
              "\n",
              "[5 rows x 104 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# Effect: Load TRAIN data, Test data from separate FILE PATH, label each column accordingly\n",
        "#         Combine them to do feature engineering and then output separate training and testing dataset\n",
        "\n",
        "def f_load_data(TRAIN_DATASET_PATH:str,\n",
        "                TEST_DATASET_PATH:str,\n",
        "                drop_na:bool = True,\n",
        "                column_name:list = None,\n",
        "                ):\n",
        "    assert(column_name != None)\n",
        "    train_dataset = pd.read_csv(TRAIN_DATASET_PATH, index_col=False, header=None,\n",
        "                              names=column_name)\n",
        "\n",
        "    test_dataset = pd.read_csv(TEST_DATASET_PATH, index_col=False, header=None,\n",
        "                             names=column_name, skiprows=1)\n",
        "  \n",
        "  \n",
        "\n",
        "    print(f'shape of train: {train_dataset.shape}')\n",
        "    print(f'shape of test: {test_dataset.shape}')\n",
        "\n",
        "    if drop_na == True:\n",
        "        train_dataset = train_dataset.replace({' ?': np.nan}).dropna()\n",
        "        test_dataset = test_dataset.replace({' ?': np.nan}).dropna()\n",
        "\n",
        "    # need ignore_index to recalculate new index\n",
        "    combined_dataset = pd.concat([train_dataset,test_dataset], ignore_index=True)\n",
        "    \n",
        "\n",
        "    # Call Feature Engineering \n",
        "    combined_dataset = adult_feature_engineering(combined_dataset)\n",
        "    \n",
        "    int_df = combined_dataset.select_dtypes(include=['int'])\n",
        "    obj_df = combined_dataset.select_dtypes(include=['object'])\n",
        "\n",
        "\n",
        "    # can do scaling in one go without looping over columns\n",
        "    int_df = (int_df - int_df.min()) / (int_df.max() - int_df.min())\n",
        "    \n",
        "    # object make dummy variables\n",
        "    encoder = ohe()\n",
        "    encoder.fit(obj_df)\n",
        "    ohe_df = pd.DataFrame(encoder.transform(obj_df).toarray())\n",
        "    \n",
        "    # join scaled interger data and dummy encoded object data frames\n",
        "    combined_dataset = int_df.join(ohe_df)\n",
        "    \n",
        "    index = train_dataset.shape[0]\n",
        "\n",
        "    train_dataset = combined_dataset.iloc[:index,:]\n",
        "    test_dataset = combined_dataset.iloc[index:,:]\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "train_dataset, test_dataset = f_load_data(TRAIN_DATASET_PATH, TEST_DATASET_PATH,  True, column_name)\n",
        "\n",
        "print(f'shape of train: {train_dataset.shape}')\n",
        "print(f'shape of test: {test_dataset.shape}')\n",
        "test_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl5TSD6BUOl9",
        "outputId": "a5a7bdd6-d342-44df-ff91-b379665dd642"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "train_dataset.index.is_unique"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss function"
      ],
      "metadata": {
        "id": "QBM1fsQsnYtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(y_pred, y_true):\n",
        "    #print(y_true)\n",
        "    #print(y_pred)\n",
        "    return np.sum(y_pred != y_true) / len(y_pred)"
      ],
      "metadata": {
        "id": "sflY4b-lna7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6isP1haSRs7"
      },
      "source": [
        "# Shekhar Algorithm "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO9d-rw0wpWa"
      },
      "source": [
        "I think the issue you have is that your starting training dataset is the whole training set, then you are adding more rows on top of that, so instead of starting out with 0 data, you starting out with thousands of data points."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn\n",
        "\n",
        "\n",
        "global init_train_size \n",
        "init_train_size = 48\n",
        "global init_valid_size\n",
        "init_valid_size = 300\n",
        "global init_pool_size\n",
        "init_pool_size = 15000\n",
        "\n",
        "\n",
        "###\n",
        "# Variables in Shekhar_sampling class:\n",
        "#   protected_label: The chosen protected label\n",
        "#   origin_train_dataset: Data points left in the training set (exclude those already sampled and used)\n",
        "#   origin_test_dataset\n",
        "#   best_classifier:\n",
        "#   empty_classifier:\n",
        "#   unique_group_label:\n",
        "#\n",
        "#   trainset: The running set that is used to train\n",
        "#   valid_set: The running set that is used to validate in training\n",
        "#   pool_set: The \n",
        "#\n",
        "###\n",
        "class Shekhar_sampling(object):\n",
        "    def __init__(\n",
        "      self,\n",
        "      classifier,\n",
        "      origin_train_dataset:DataFrame = None,\n",
        "      origin_test_dataset:DataFrame = None,\n",
        "      protected_label:str = None,\n",
        "      ):\n",
        "\n",
        "        assert(protected_label != None)\n",
        "        self.protected_label = protected_label\n",
        "        self.origin_train_dataset = origin_train_dataset.copy()\n",
        "        self.origin_test_dataset = origin_test_dataset.copy()\n",
        "        self.best_classifier = sklearn.base.clone(classifier)\n",
        "        self.empty_classifier = sklearn.base.clone(classifier)\n",
        "\n",
        "        self.unique_group_label = origin_train_dataset[protected_label].unique()\n",
        "\n",
        "        # construct the initial training set and remove them from the original set\n",
        "        #self.trainset = self.origin_train_dataset.sample(n = init_train_size)\n",
        "        #self.origin_train_dataset.drop(index=self.trainset.index, inplace=True)\n",
        "\n",
        "\n",
        "        # construct the validation set \n",
        "        self.valid_set = self.origin_train_dataset.sample(n = init_valid_size)\n",
        "        self.origin_train_dataset.drop(index=self.valid_set.index, inplace=True)\n",
        "        self.valid_set.reset_index(drop=True, inplace=True)\n",
        "        self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # construct the pool set for training process\n",
        "        self.pool_set = self.origin_train_dataset.sample(n = init_pool_size)\n",
        "        self.origin_train_dataset.drop(index=self.pool_set.index, inplace=True)\n",
        "        self.pool_set.reset_index(drop=True, inplace=True)\n",
        "        self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        \n",
        "        # sample 4 data points for the initial training set\n",
        "        tmp_set = self.origin_train_dataset[self.origin_train_dataset[self.protected_label] == 0]\n",
        "        self.trainset = tmp_set[tmp_set['label'] == 0].sample()\n",
        "        tmp_set.reset_index(drop=True, inplace=True)\n",
        "        self.trainset.reset_index(drop=True, inplace=True)\n",
        "        \n",
        "        smp = tmp_set[tmp_set['label'] == 1].sample()\n",
        "        self.trainset = pd.concat([self.trainset, smp], ignore_index=True)\n",
        "        self.trainset.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        tmp_set = self.origin_train_dataset[self.origin_train_dataset[self.protected_label] == 1]\n",
        "        smp = tmp_set[tmp_set['label'] == 0].sample()\n",
        "        self.trainset = pd.concat([self.trainset, smp], ignore_index=True)\n",
        "        tmp_set.reset_index(drop=True, inplace=True)\n",
        "        self.trainset.reset_index(drop=True, inplace=True)\n",
        "        \n",
        "        smp = tmp_set[tmp_set['label'] == 1].sample()\n",
        "        self.trainset = pd.concat([self.trainset, smp], ignore_index=True)\n",
        "        self.trainset.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "        # Need to reset index to keep everything in order\n",
        "        self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "        self.trainset.reset_index(drop=True, inplace=True)\n",
        "        self.valid_set.reset_index(drop=True, inplace=True)\n",
        "        #display(self.trainset)\n",
        "        #display(self.origin_train_dataset)\n",
        "\n",
        "    def update_ez(self, zt, dvc = 3, delta = 0.05):\n",
        "        self.ez[zt] = 2*(2*dvc*np.log(2*np.e*self.Nz[zt]/dvc))**(1/2) + 2*np.log(2*\n",
        "            self.Nz[zt]*np.pi*4/(3*delta))\n",
        "    \n",
        "    def update_pi(self):\n",
        "        total = 0\n",
        "        for key, val in self.Nz.items():\n",
        "            total += val\n",
        "        for key, val in self.pi.items():\n",
        "            self.pi[key] = self.Nz[key]/total\n",
        "    \n",
        "    def update_Ut(self, loss_func, C=1):\n",
        "        loss = 0\n",
        "\n",
        "        total = 0\n",
        "        for key, val in self.pi.items():\n",
        "            total += self.pi[key]*self.ez[key]\n",
        "\n",
        "        for key, val in self.Ut.items():\n",
        "            y_pred = self.c_classifier.predict(self.Dz[key])\n",
        "            loss = loss_func(y_pred, self.Dz_y[key].to_numpy().reshape(-1))\n",
        "            self.Ut[key] = loss/len(self.Dz[key]) + self.ez[key] + (2*C/self.pi[key])*total\n",
        "    \n",
        "    \n",
        "    # Make a new class for Shekhar because unlike Abernethy, it needs to modify\n",
        "    # the test dataset during train time\n",
        "    \n",
        "    # Question: sometimes it will show error as y only has 1 unique value\n",
        "    def train(\n",
        "        self,\n",
        "        loss_func,\n",
        "        C,\n",
        "        chosen_metric:str = 'Overall_accuracy',\n",
        "        quiet_mode:bool = True,\n",
        "        stop_iter_num:int = 1000,\n",
        "        m = 2\n",
        "        ):\n",
        "        self.scores_list = []\n",
        "        self.metric_result_list = []\n",
        "        self.run_out_time = {}\n",
        "        best_score = -1\n",
        "\n",
        "        train_data = self.trainset.copy()\n",
        "\n",
        "        full_test_result = {}\n",
        "\n",
        "        self.Dz = {}\n",
        "        self.Dz_y = {}\n",
        "        Dt_y = train_data['label']\n",
        "        Dt = train_data.drop('label', axis=1)\n",
        "\n",
        "        #Dt_y.columns = Dt_y.columns.astype(str)\n",
        "        #Dt.columns = Dt.columns.astype(str)\n",
        "        \n",
        "\n",
        "        self.Nz = {}\n",
        "        self.pi = {}\n",
        "        self.Ut = {}\n",
        "        self.ez = {}\n",
        "\n",
        "        y_test = self.valid_set['label']\n",
        "        X_test = self.valid_set.drop('label', axis=1)\n",
        "\n",
        "        full_test_y = self.origin_test_dataset['label']\n",
        "        full_test_X = self.origin_test_dataset.drop('label', axis=1)\n",
        "\n",
        "        #y_test.columns = y_test.columns.astype(str)\n",
        "        #X_test.columns = X_test.columns.astype(str)\n",
        "        \n",
        "        \n",
        "        \n",
        "        for tmp_label in self.unique_group_label:\n",
        "            self.Nz[tmp_label] = 0\n",
        "            self.Ut[tmp_label] = 0\n",
        "            self.ez[tmp_label] = 0\n",
        "            self.pi[tmp_label] = 0\n",
        "            self.Dz[tmp_label] = pd.DataFrame()\n",
        "            self.Dz_y[tmp_label] = pd.DataFrame()\n",
        "\n",
        "\n",
        "        tmp_label = self.unique_group_label\n",
        "\n",
        "        for t in range(1, int(stop_iter_num) + 1):\n",
        "            minimum_set_label = min(self.Nz.keys(), key=(lambda k:self.Nz[k]))\n",
        "            if t <= m:\n",
        "                zt = tmp_label[t-1]\n",
        "            # get the key of the minimum and the minimum value\n",
        "            elif self.Nz[minimum_set_label] < t**(1/2):\n",
        "                zt = minimum_set_label\n",
        "            \n",
        "            #########\n",
        "            # Sometimes the test Dataset only contains 1 label, making it impossible to fit\n",
        "            #elif len(Dt_y[0].unique()) <= 1:\n",
        "            #    i = np.random.randint(0,m)\n",
        "            #    \n",
        "            #    zt = tmp_label[i]\n",
        "            ########\n",
        "            else:\n",
        "                self.update_Ut(loss_func, C=C)\n",
        "                zt = max(self.Ut.keys(), key=(lambda k:self.Ut[k]))\n",
        "\n",
        "            sample_set = self.pool_set[self.pool_set[self.protected_label] == zt]\n",
        "            if len(sample_set) < 2 and self.run_out_time[zt] == -1:\n",
        "                print(f'Run out of sample for group {zt} at iteration {t}')\n",
        "                self.run_out_time[zt] = t\n",
        "\n",
        "            # Sample 2 data points and add them to the training dataset\n",
        "            if len(sample_set) > 2:\n",
        "                smp_train = self.pool_set[self.pool_set[self.protected_label] == zt].sample()\n",
        "                \n",
        "                self.pool_set.drop(index=smp_train.index, inplace=True)\n",
        "                self.pool_set.reset_index(drop=True, inplace=True)\n",
        "                Dt_y = pd.concat([Dt_y, smp_train['label']], ignore_index=True)\n",
        "                Dt = pd.concat([Dt, smp_train.drop('label', axis=1)], ignore_index=True)\n",
        "                self.Nz[zt] += 1\n",
        "\n",
        "                smp_test = self.pool_set[self.pool_set[self.protected_label] == zt].sample()\n",
        "                \n",
        "                self.pool_set.drop(index=smp_test.index, inplace=True)\n",
        "                self.pool_set.reset_index(drop=True, inplace=True)\n",
        "                self.Dz_y[zt] = pd.concat([self.Dz_y[zt], smp_test['label']], ignore_index=True)\n",
        "                self.Dz[zt] = pd.concat([self.Dz[zt], smp_test.drop('label', axis=1)], ignore_index=True)\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "            if len(self.pool_set) < 2:\n",
        "                print(f'Run out of All sample at {t}')\n",
        "            \n",
        "            self.update_ez(zt)\n",
        "            self.update_pi()\n",
        "\n",
        "            self.c_classifier = sklearn.base.clone(self.empty_classifier)\n",
        "            #display(Dt_y)\n",
        "\n",
        "            ###\n",
        "            # Train the model and make predictions\n",
        "            ###\n",
        "            if len(Dt_y.unique()) > 1:\n",
        "                #display(Dt)\n",
        "                #print(t)\n",
        "                self.c_classifier.fit(Dt, Dt_y)\n",
        "\n",
        "                # get test score and fairness violation\n",
        "                y_pred = self.c_classifier.predict(X_test)\n",
        "                c_score = self.c_classifier.score(X_test, y_test)\n",
        "                self.scores_list.append(c_score)\n",
        "\n",
        "                metric = Fairness_metric(X_test[self.protected_label], true_label=y_test, predict_label=y_pred)\n",
        "                #print(X_test[self.protected_label])\n",
        "                \n",
        "                metric.process()\n",
        "                all_violation = metric.calculate_all()\n",
        "                self.metric_result_list.append(all_violation)\n",
        "\n",
        "                ###\n",
        "                #\n",
        "                # Run the algorithm on the test dataset\n",
        "                if t % 500 == 0: \n",
        "                    test_pred = self.c_classifier.predict(full_test_X)\n",
        "                    test_metric = Fairness_metric(full_test_X[self.protected_label], true_label=full_test_y, predict_label=test_pred)\n",
        "                    metric.process()\n",
        "                    all_violation = metric.calculate_all()\n",
        "                    full_test_result[t] = [t, loss_func(test_pred, full_test_y), \n",
        "                        abs(all_violation['Demographic_Parity'][2][0]), abs(all_violation['Equal_Opportunity'][2][0]),\n",
        "                        abs(all_violation['Equal_Odds'][2][0]), abs(all_violation['Overall_accuracy'][2][0])] \n",
        "\n",
        "        \n",
        "\n",
        "        # Question: How to define the best classifier?\n",
        "        return self.best_classifier, self.scores_list, self.metric_result_list, full_test_result\n",
        "  \n",
        "    def predict(\n",
        "      self,\n",
        "      ):\n",
        "      return None\n",
        "  \n",
        "    def save(\n",
        "      self,\n",
        "      path:str):\n",
        "      return None\n",
        "  \n",
        "    def load(\n",
        "      self,\n",
        "      path:str):\n",
        "      return None"
      ],
      "metadata": {
        "id": "EE9jfu7aOeov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#classifier = LogisticRegression()\n",
        "#model = Shekhar_sampling(classifier,  train_dataset, test_dataset, 'group')\n",
        "#best_classifier, scores_list, metric_result_list, full_test_result = model.train(loss_func, C=0.1, quiet_mode = False, chosen_metric='Demographic_Parity', stop_iter_num = 1000)"
      ],
      "metadata": {
        "id": "B9hlQTLU5Q_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "vio_list = []\n",
        "for dict_i in metric_result_list:\n",
        "  vio_list.append(abs(dict_i['Equal_Opportunity'][2]))\n",
        "plt.plot(vio_list)\n",
        "'''"
      ],
      "metadata": {
        "id": "Nan8P6JT9vUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3195467-0f37-4793-cd66-73c0b2a0b06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nvio_list = []\\nfor dict_i in metric_result_list:\\n  vio_list.append(abs(dict_i['Equal_Opportunity'][2]))\\nplt.plot(vio_list)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Process"
      ],
      "metadata": {
        "id": "BZSRDLsOOYbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Abernethy_p_sampling(object):\n",
        "    def __init__(self,\n",
        "                 p,\n",
        "                 ):\n",
        "        self.p = p\n",
        "    \n",
        "    def get_group(self,\n",
        "                  all_violation, \n",
        "                  chosen_metric):\n",
        "        t_random_num = np.random.uniform()\n",
        "        if t_random_num < self.p:\n",
        "            return all_violation[chosen_metric][1]\n",
        "        else:\n",
        "            return None\n"
      ],
      "metadata": {
        "id": "yKvjl7Aa87Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDRdZD3lYLt8"
      },
      "outputs": [],
      "source": [
        "# Template model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn\n",
        "\n",
        "'''\n",
        "global init_train_size \n",
        "init_train_size = 48\n",
        "global init_valid_size\n",
        "init_valid_size = 300\n",
        "global init_pool_size\n",
        "init_pool_size = 2000\n",
        "'''\n",
        "\n",
        "init_pool_size = 1000\n",
        "\n",
        "\n",
        "###\n",
        "# Variables in Training_General class:\n",
        "#   protected_label: The chosen protected label\n",
        "#   initial_set_num: The size of the initial training set\n",
        "#   origin_train_dataset: Data points left in the training set (exclude those already sampled and used)\n",
        "#   origin_test_dataset\n",
        "#   best_classifier:\n",
        "#   empty_classifier:\n",
        "#   unique_group_label:\n",
        "#\n",
        "#   trainset: The running set that is used to train\n",
        "#   valid_set: The running set that is used to validate in training\n",
        "#   pool_set: The \n",
        "#\n",
        "###\n",
        "class Training_General(object):\n",
        "    def __init__(\n",
        "      self,\n",
        "      classifier,\n",
        "      initial_set_num:int = 48,\n",
        "      origin_train_dataset:DataFrame = None,\n",
        "      origin_test_dataset:DataFrame = None,\n",
        "      protected_label:str = None,\n",
        "      pool_size = 1000,\n",
        "      ):\n",
        "\n",
        "        assert(protected_label != None)\n",
        "        #assert(0 < p <=1)\n",
        "        self.protected_label = protected_label\n",
        "        #self.p = p\n",
        "        self.origin_train_dataset = origin_train_dataset.copy()\n",
        "        self.origin_test_dataset = origin_test_dataset.copy()\n",
        "        self.best_classifier = sklearn.base.clone(classifier)\n",
        "        self.empty_classifier = sklearn.base.clone(classifier)\n",
        "\n",
        "        # construct the initial training set and remove them from the original training set\n",
        "        self.trainset = self.origin_train_dataset.sample(n = initial_set_num)\n",
        "        self.origin_train_dataset.drop(index=self.trainset.index, inplace=True)\n",
        "\n",
        "        self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "        self.trainset.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # construct the validation set for training process\n",
        "        self.valid_set = self.origin_train_dataset.sample(n = init_valid_size)\n",
        "        self.origin_train_dataset.drop(index=self.valid_set.index, inplace=True)\n",
        "\n",
        "        self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "        self.valid_set.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # construct the pool set for training process\n",
        "        self.pool_set = self.origin_train_dataset.sample(n = init_pool_size)\n",
        "        self.origin_train_dataset.drop(index=self.pool_set.index, inplace=True)\n",
        "\n",
        "        # Need to reset index to keep everything in order\n",
        "        self.origin_train_dataset.reset_index(drop=True, inplace=True)\n",
        "        self.pool_set.reset_index(drop=True, inplace=True)\n",
        "        #display(self.trainset)\n",
        "        #display(self.origin_train_dataset)\n",
        "\n",
        "    # Sampling algorithm needs to be a class like Abernethy_p_sampling \n",
        "    def train(\n",
        "      self,\n",
        "      sampling_algorithm,\n",
        "      loss_func = loss_func,\n",
        "      chosen_metric:str = 'Overall_accuracy',\n",
        "      quiet_mode:bool = True,\n",
        "      stop_iter_num:int = 1000,\n",
        "      ):\n",
        "      ###\n",
        "      # X_test, y_test: the validation set\n",
        "      # full_test_X, full_test_y: the complete test set for 500/1000 checkpoint\n",
        "      ###\n",
        "\n",
        "\n",
        "        self.scores_list = []\n",
        "        self.metric_result_list = []\n",
        "        self.run_out_time = {}\n",
        "        best_score = -1\n",
        "\n",
        "        unique_p_label = self.origin_train_dataset[self.protected_label].unique()\n",
        "        for i in unique_p_label:\n",
        "            self.run_out_time[i] = -1\n",
        "\n",
        "        stop_training = False\n",
        "        \n",
        "        # construct the X_train, y_train, X_test, y_test\n",
        "        train_data = self.trainset.copy()\n",
        "        y_train = train_data['label']\n",
        "        X_train = train_data.drop('label', axis=1)\n",
        "        \n",
        "        val_data = self.valid_set.copy()\n",
        "        y_test = val_data['label']\n",
        "        X_test = val_data.drop('label', axis=1)\n",
        "\n",
        "\n",
        "        full_test_y = self.origin_test_dataset['label']\n",
        "        full_test_X = self.origin_test_dataset.drop('label', axis=1)\n",
        "\n",
        "        ###\n",
        "        #\n",
        "        ###\n",
        "        #X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y)\n",
        "\n",
        "        # fit the initial model and output the prediction\n",
        "        c_classifier = sklearn.base.clone(self.empty_classifier)\n",
        "        c_classifier.fit(X_train, y_train)\n",
        "        y_pred = c_classifier.predict(X_test)\n",
        "\n",
        "        # obtain score and fairness violation\n",
        "        initial_score = loss_func(y_pred, y_test)\n",
        "        #################\n",
        "\n",
        "        best_score = initial_score\n",
        "        self.scores_list.append(initial_score)\n",
        "        metric = Fairness_metric(X_test[self.protected_label], true_label=y_test, predict_label=y_pred)\n",
        "        metric.process()\n",
        "        all_violation = metric.calculate_all()\n",
        "        self.metric_result_list.append(all_violation)\n",
        "        \n",
        "        label_to_sample = sampling_algorithm.get_group(all_violation, chosen_metric)\n",
        "        c_vio = all_violation[chosen_metric][2]\n",
        "        if quiet_mode == False: \n",
        "            print(f'Initial Score: {initial_score}, Initial violation: {c_vio}, Initial biased group: {label_to_sample}')\n",
        "\n",
        "        #display(train_data)\n",
        "        # sample from biased group\n",
        "        if label_to_sample == None:\n",
        "            smp = self.pool_set.sample()\n",
        "            self.pool_set.drop(index=smp.index, inplace=True)\n",
        "            self.pool_set.reset_index(drop=True, inplace=True)\n",
        "            train_data = pd.concat([train_data, smp], ignore_index=True)\n",
        "        else:\n",
        "            smp = self.pool_set[self.pool_set[self.protected_label] == label_to_sample].sample()\n",
        "            self.pool_set.drop(index=smp.index, inplace=True)\n",
        "            self.pool_set.reset_index(drop=True, inplace=True)\n",
        "            train_data = pd.concat([train_data, smp], ignore_index=True)\n",
        "            \n",
        "        iteration = 2\n",
        "\n",
        "        ###\n",
        "        # Stores the full test result every 500 iteration\n",
        "        ###\n",
        "        full_test_result = {}\n",
        "\n",
        "        while stop_training == False:\n",
        "            y_train = train_data['label']\n",
        "            X_train = train_data.drop('label', axis=1)\n",
        "        \n",
        "            # We use the same validation set for the entire training process\n",
        "            #val_data = self.valid_set.copy()\n",
        "            #y_test = val_data['label']\n",
        "            #X_test = val_data.drop('label', axis=1)\n",
        "\n",
        "\n",
        "            self.X_train_backup, self.y_train_backup = X_train, y_train\n",
        "\n",
        "            # fit the initial model and output the prediction\n",
        "            c_classifier = sklearn.base.clone(self.empty_classifier)\n",
        "            \n",
        "            c_classifier.fit(X_train, y_train)\n",
        "            y_pred = c_classifier.predict(X_test)\n",
        "\n",
        "            # obtain score and fairness violation\n",
        "            c_score = loss_func(y_pred, y_test)\n",
        "            self.scores_list.append(c_score)\n",
        "            \n",
        "            # Fairness violation\n",
        "            metric = Fairness_metric(X_test[self.protected_label], true_label=y_test, predict_label=y_pred)\n",
        "            #print(X_test[self.protected_label])\n",
        "            \n",
        "            metric.process()\n",
        "            all_violation = metric.calculate_all()\n",
        "            self.metric_result_list.append(all_violation)\n",
        "\n",
        "            # save the latest classifier\n",
        "            self.last_classifier = c_classifier\n",
        "\n",
        "            label_to_sample = sampling_algorithm.get_group(all_violation, chosen_metric)\n",
        "            c_vio = all_violation[chosen_metric][2]\n",
        "            if quiet_mode == False and iteration  % 500 == 0: \n",
        "                print(f'Score: {c_score}, violation: {c_vio}, biased group: {label_to_sample}')\n",
        "            \n",
        "            ###\n",
        "            #\n",
        "            # Run the algorithm on the test dataset\n",
        "            if iteration % 500 == 0: \n",
        "                test_pred = c_classifier.predict(full_test_X)\n",
        "                test_metric = Fairness_metric(full_test_X[self.protected_label], true_label=full_test_y, predict_label=test_pred)\n",
        "                metric.process()\n",
        "                all_violation = metric.calculate_all()\n",
        "                full_test_result[iteration] = [iteration, loss_func(test_pred, full_test_y), \n",
        "                        abs(all_violation['Demographic_Parity'][2][0]), abs(all_violation['Equal_Opportunity'][2][0]),\n",
        "                   abs(all_violation['Equal_Odds'][2][0]), abs(all_violation['Overall_accuracy'][2][0])] \n",
        "\n",
        "\n",
        "            #print(all_violation)\n",
        "\n",
        "            \n",
        "            if c_score > best_score:\n",
        "                best_score = c_score\n",
        "                self.best_classifier = c_classifier\n",
        "\n",
        "            # sample from biased group\n",
        "            biased_set = self.pool_set[self.pool_set[self.protected_label] == label_to_sample]\n",
        "            if label_to_sample != None and len(biased_set) == 0 and self.run_out_time[label_to_sample] == -1:\n",
        "                print(f'Run out of sample for group {label_to_sample} at iteration {iteration}')\n",
        "                self.run_out_time[label_to_sample] = iteration\n",
        "\n",
        "            if label_to_sample != None and len(biased_set) != 0:\n",
        "                smp = self.pool_set[self.pool_set[self.protected_label] == label_to_sample].sample()\n",
        "                \n",
        "                self.pool_set.drop(index=smp.index, inplace=True)\n",
        "                self.pool_set.reset_index(drop=True, inplace=True)\n",
        "                train_data = pd.concat([train_data, smp], ignore_index=True)\n",
        "            else:\n",
        "                smp = self.pool_set.sample()\n",
        "                self.pool_set.drop(index=smp.index, inplace=True)\n",
        "                self.pool_set.reset_index(drop=True, inplace=True)\n",
        "                train_data = pd.concat([train_data, smp], ignore_index=True)\n",
        "\n",
        "            if len(self.pool_set) == 0 or iteration >= stop_iter_num:\n",
        "                stop_training = True\n",
        "            iteration += 1\n",
        "\n",
        "\n",
        "        # How to define the best classifier?\n",
        "        return self.best_classifier, self.last_classifier, self.scores_list, self.metric_result_list, full_test_result\n",
        "  \n",
        "    def predict(\n",
        "      self,\n",
        "      ):\n",
        "      return None\n",
        "  \n",
        "    def save(\n",
        "      self,\n",
        "      path:str):\n",
        "      return None\n",
        "  \n",
        "    def load(\n",
        "      self,\n",
        "      path:str):\n",
        "      return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vxahce5WMA9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358fed78-6451-4183-ba25-df93eed8372b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nclassifier = LogisticRegression( max_iter = 500)\\nsampling_algorithm = Abernethy_p_sampling(0.5)\\nmodel = Training_General(classifier, 48, train_dataset, test_dataset, 'group')\\nlen(train_dataset)\\nbest_classifier, last_classifier, scores_list, metric_result_list, full_test_result = model.train(sampling_algorithm, quiet_mode = False, chosen_metric='Demographic_Parity', stop_iter_num = 1000)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "'''\n",
        "classifier = LogisticRegression( max_iter = 500)\n",
        "sampling_algorithm = Abernethy_p_sampling(0.5)\n",
        "model = Training_General(classifier, 48, train_dataset, test_dataset, 'group')\n",
        "len(train_dataset)\n",
        "best_classifier, last_classifier, scores_list, metric_result_list, full_test_result = model.train(sampling_algorithm, quiet_mode = False, chosen_metric='Demographic_Parity', stop_iter_num = 1000)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plt.plot(scores_list)"
      ],
      "metadata": {
        "id": "dHgFatKtobgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEHTk8w67tmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6adb657-13e6-476a-a7a8-ea1742de23cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nvio_list = []\\nfor dict_i in metric_result_list:\\n  vio_list.append(abs(dict_i['Equal_Opportunity'][2]))\\nplt.plot(vio_list)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "'''\n",
        "vio_list = []\n",
        "for dict_i in metric_result_list:\n",
        "  vio_list.append(abs(dict_i['Equal_Opportunity'][2]))\n",
        "plt.plot(vio_list)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate plot"
      ],
      "metadata": {
        "id": "c0NT3Ed28deS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate iteration versus fair dataframe \n",
        "def Abernethy_generate_t_vs_fair(\n",
        "    classifier,\n",
        "    num_trials,\n",
        "    param_list,\n",
        "    iter_num,\n",
        "    chosen_metric,\n",
        "):\n",
        "    res = []\n",
        "    for p in param_list:\n",
        "        print(f'p:{p}')\n",
        "        for i in range(num_trials):\n",
        "            print(f'p:{p}, i={i}')\n",
        "            sampling_algorithm = Abernethy_p_sampling(p)\n",
        "\n",
        "            model = Training_General(classifier=classifier,  initial_set_num=48, \n",
        "                origin_train_dataset=train_dataset, origin_test_dataset = test_dataset, protected_label='group')\n",
        "            best_classifier, last_classifier, scores_list, metric_result_list, full_test_result = model.train(sampling_algorithm,\n",
        "                    quiet_mode = True, chosen_metric=chosen_metric, stop_iter_num = iter_num)\n",
        "            for iteration, value in full_test_result.items():\n",
        "                tmp_row = np.insert(value, 0, p, axis=0)\n",
        "                res.append(tmp_row)\n",
        "            #tmp_dict = metric_result_list[-1]\n",
        "            #res.append([p, abs(tmp_dict['Demographic_Parity'][2]), abs(tmp_dict['Equal_Opportunity'][2]),\n",
        "            #       abs(tmp_dict['Equal_Odds'][2]), abs(tmp_dict['Overall_accuracy'][2])])\n",
        "        tmp_tmp = pd.DataFrame(res, columns=['p', 'checkpoint', 'train_error', 'Demographic_Parity', 'Equal_Opportunity', 'Equal_Odds', 'Overall_accuracy'])\n",
        "        tmp_tmp.to_csv(RESULT_SAVE_PATH + f'/Abernethy_t_vs_fair_tmp_tmp_{p}.csv')\n",
        "    res = pd.DataFrame(res, columns=['p', 'checkpoint', 'train_error', 'demographic parity', 'equal_opportunity', 'equal odds', 'overall accuracy'])\n",
        "    return res\n",
        "\n",
        "# Generate the error vs fairness violation plot\n",
        "def Abernethy_generate_plot(\n",
        "    classifier,\n",
        "    sampling_algorithm,\n",
        "    num_trials,\n",
        "    param_list,\n",
        "    iter_num,\n",
        "    chosen_metric,\n",
        "):\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    for p in param_list:\n",
        "\n",
        "        ###\n",
        "        # Average the score over num_trials models with the same parameter\n",
        "        ###\n",
        "        tmp_x_list = []\n",
        "        tmp_y_list = []\n",
        "        for i in range(num_trials):\n",
        "            model = Training_General(classifier=classifier, p=p, initial_set_num=48, \n",
        "                origin_train_dataset=train_dataset, origin_test_dataset = test_dataset, protected_label='group')\n",
        "            best_classifier, last_classifier, scores_list, metric_result_list = model.train(sampling_algorithm, quiet_mode = False, chosen_metric=chosen_metric, stop_iter_num = iter_num)\n",
        "            tmp_dict = metric_result_list[-1]\n",
        "            tmp_x_list.append(abs(tmp_dict['Overall_accuracy'][2]))\n",
        "            tmp_y_list.append(abs(tmp_dict[chosen_metric][2]))\n",
        "        \n",
        "        x_list.append(np.average(tmp_x_list))\n",
        "        y_list.append(np.average(tmp_y_list))\n",
        "    plt.plot(x_list, y_list)\n",
        "\n",
        "    #"
      ],
      "metadata": {
        "id": "wEA3guet8lK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(RESULT_SAVE_PATH + '/Abernethy_t_vs_fair.csv')\n",
        "p_list = np.linspace(0.4, 1, 13)\n",
        "classifier = LogisticRegression()\n",
        "df = Abernethy_generate_t_vs_fair(classifier,  10, p_list, 1000, 'Equal_Opportunity')\n",
        "df.to_csv(RESULT_SAVE_PATH + '/Abernethy_t_vs_fair.csv')\n",
        "\n",
        "#Abernethy_generate_plot(classifier, sampling_algorithm, 10, [0.1, 0.2, 0.3, 0.4, 0.5], 100, 'Equal_Opportunity')"
      ],
      "metadata": {
        "id": "X18cAEbtfN1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate iteration versus fair dataframe \n",
        "def Shekhar_generate_t_vs_fair(\n",
        "    classifier,\n",
        "    num_trials,\n",
        "    param_list,\n",
        "    iter_num,\n",
        "    chosen_metric,\n",
        "):\n",
        "    res = []\n",
        "    for C in param_list:\n",
        "        for i in range(num_trials):\n",
        "\n",
        "            model = Shekhar_sampling(classifier,  train_dataset, test_dataset, 'group')\n",
        "            \n",
        "            best_classifier, scores_list, metric_result_list, full_test_result = model.train(loss_func,\n",
        "                C=C, quiet_mode = True, chosen_metric=chosen_metric, stop_iter_num = 1000)            \n",
        "            for iteration, value in full_test_result.items():\n",
        "                tmp_row = np.insert(value, 0, C, axis=0)\n",
        "                res.append(tmp_row)\n",
        "            #tmp_dict = metric_result_list[-1]\n",
        "            #res.append([p, abs(tmp_dict['Demographic_Parity'][2]), abs(tmp_dict['Equal_Opportunity'][2]),\n",
        "            #       abs(tmp_dict['Equal_Odds'][2]), abs(tmp_dict['Overall_accuracy'][2])])\n",
        "        tmp_tmp = pd.DataFrame(res, columns=['C', 'checkpoint', 'train_error', 'Demographic_Parity', 'Equal_Opportunity', 'Equal_Odds', 'Overall_accuracy'])\n",
        "        tmp_tmp.to_csv(RESULT_SAVE_PATH + '/Shekhar_t_vs_fair_tmp_tmp.csv')\n",
        "    res = pd.DataFrame(res, columns=['C', 'checkpoint', 'train_error', 'Demographic_Parity', 'Equal_Opportunity', 'Equal_Odds', 'Overall_accuracy'])\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "alAv4M5QwQgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = LogisticRegression()\n",
        "C_list = np.linspace(0.4, 1, 13)\n",
        "\n",
        "model = Shekhar_sampling(classifier,  train_dataset, test_dataset, 'group')\n",
        "df = Shekhar_generate_t_vs_fair(classifier,  10, C_list, 1000, 'Equal_Opportunity')\n",
        "df.to_csv(RESULT_SAVE_PATH + '/Shekhar_t_vs_fair.csv')\n"
      ],
      "metadata": {
        "id": "jrHRVg_iwQtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Code"
      ],
      "metadata": {
        "id": "1qriYYMy8a9v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqvR4wiNNAq0"
      },
      "outputs": [],
      "source": [
        "\n",
        "smp = train_dataset[train_dataset[\"sex\"]== 0].sample()\n",
        "print(smp)\n",
        "index = smp.index\n",
        "print(train_dataset.iloc[index])\n",
        "#print(smp.index)\n",
        "train_dataset.drop(index=smp.index, inplace=True)\n",
        "train_dataset.reset_index(drop=True, inplace=True)\n",
        "print(train_dataset.iloc[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSyc5V-qRwEY"
      },
      "outputs": [],
      "source": [
        "train_dataset.index.is_unique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJws2nDZSZkw"
      },
      "source": [
        "# Fairness Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xlte9p0KScjZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmNlFx3OTsrB"
      },
      "outputs": [],
      "source": [
        "#self.true_data = train_dataset[[protected_col,label_col]].to_numpy()\n",
        "# true_data is |protected_label | true_y |\n",
        "\n",
        "class Fairness_metric(object):\n",
        "  def __init__(\n",
        "      self,\n",
        "      protected_label:list,\n",
        "      true_label:list,\n",
        "      predict_label:list,\n",
        "      ):\n",
        "    assert(len(protected_label) == len(true_label))\n",
        "    assert(len(predict_label) == len(true_label))\n",
        "    self.protected_label = np.array(protected_label)\n",
        "    self.predict_label = np.array(predict_label)\n",
        "    self.true_label = np.array(true_label)\n",
        "    self.res_mat = {}\n",
        "    # res_mat: \n",
        "    # key : protected label \n",
        "    # val: [TP, FP, FN, TN]\n",
        "    \n",
        "  def process(self):\n",
        "    for key in np.unique(self.protected_label):\n",
        "      self.res_mat[key] = [0,0,0,0]\n",
        "      # TP, FP, FN, TN\n",
        "    for i in range(len(self.predict_label)):\n",
        "      tmp_prot = self.protected_label[i]\n",
        "      tmp_true = self.true_label[i]\n",
        "      tmp_pred = self.predict_label[i]\n",
        "      if tmp_true == tmp_pred and tmp_true == 1:\n",
        "        # TP\n",
        "        self.res_mat[tmp_prot][0] +=1\n",
        "      elif tmp_true == 1 and tmp_pred == 0:\n",
        "        # FN\n",
        "        self.res_mat[tmp_prot][2] +=1\n",
        "      elif tmp_true == 0 and tmp_pred == 1:\n",
        "        # FP\n",
        "        self.res_mat[tmp_prot][1] +=1\n",
        "      elif tmp_true == 0 and tmp_pred == 0:\n",
        "        # TN\n",
        "        self.res_mat[tmp_prot][3] +=1\n",
        "      else:\n",
        "        print('Incorrect Code')\n",
        "\n",
        "  # This is for two protected labels, can be adapted to multiple protected labels\n",
        "  # output an dict of scores of all labels and a list containing biased group with\n",
        "  # its violation\n",
        "  def Demographic_Parity(\n",
        "      self,\n",
        "      ):\n",
        "    res = {}\n",
        "    # [label : score]\n",
        "    for label, tmp_list in self.res_mat.items():\n",
        "      TP = tmp_list[0]\n",
        "      FP = tmp_list[1]\n",
        "      FN = tmp_list[2]\n",
        "      TN = tmp_list[3]\n",
        "      tot = np.sum(tmp_list)\n",
        "      res[label] = [TP+FP]/tot\n",
        "\n",
        "    min_violation = 99\n",
        "    min_label = None\n",
        "\n",
        "    for label, score in res.items():\n",
        "      for label_2, score_2 in res.items():\n",
        "        if label == label_2:\n",
        "          pass\n",
        "        vio = score-score_2\n",
        "        if vio < min_violation:\n",
        "          min_violation = vio\n",
        "          min_label = label\n",
        "    \n",
        "    assert(min_violation != 99)\n",
        "    assert(-1 <= min_violation <= 0)\n",
        "    assert(min_label != None)\n",
        "    return res , min_label, min_violation\n",
        "  \n",
        "  def Equal_Opportunity(\n",
        "      self,\n",
        "      ):\n",
        "    res = {}\n",
        "    # [label : score]\n",
        "    for label, tmp_list in self.res_mat.items():\n",
        "      \n",
        "      TP = tmp_list[0]\n",
        "      FP = tmp_list[1]\n",
        "      FN = tmp_list[2]\n",
        "      TN = tmp_list[3]\n",
        "      tot = np.sum(tmp_list)\n",
        "      res[label] =[TP]/tot\n",
        "\n",
        "    min_violation = 99\n",
        "    min_label = None\n",
        "\n",
        "    for label, score in res.items():\n",
        "      for label_2, score_2 in res.items():\n",
        "        if label == label_2:\n",
        "          pass\n",
        "        vio = score-score_2\n",
        "        if vio < min_violation:\n",
        "          min_violation = vio\n",
        "          min_label = label\n",
        "    \n",
        "    assert(min_violation != 99)\n",
        "    assert(-1 <= min_violation <= 0)\n",
        "    assert(min_label != None)\n",
        "    return res , min_label, min_violation\n",
        "  \n",
        "  def Equal_Odds(\n",
        "      self,\n",
        "      ):\n",
        "    res = {}\n",
        "    # [label : score]\n",
        "    for label, tmp_list in self.res_mat.items():\n",
        "      TP = tmp_list[0]\n",
        "      FP = tmp_list[1]\n",
        "      FN = tmp_list[2]\n",
        "      TN = tmp_list[3]\n",
        "      tot = np.sum(tmp_list)\n",
        "      res[label] =[TP+FP]/tot\n",
        "\n",
        "    min_violation = 99\n",
        "    min_label = None\n",
        "\n",
        "    for label, score in res.items():\n",
        "      for label_2, score_2 in res.items():\n",
        "        if label == label_2:\n",
        "          pass\n",
        "        vio = score-score_2\n",
        "        if vio < min_violation:\n",
        "          min_violation = vio\n",
        "          min_label = label\n",
        "    \n",
        "    assert(min_violation != 99)\n",
        "    assert(-1 <= min_violation <= 0)\n",
        "    assert(min_label != None)\n",
        "    return res , min_label, min_violation\n",
        "\n",
        "  def Overall_accuracy(\n",
        "      self,\n",
        "      ):\n",
        "    res = {}\n",
        "    # [label : score]\n",
        "    for label, tmp_list in self.res_mat.items():\n",
        "      TP = tmp_list[0]\n",
        "      FP = tmp_list[1]\n",
        "      FN = tmp_list[2]\n",
        "      TN = tmp_list[3]\n",
        "      tot = np.sum(tmp_list)\n",
        "      res[label] =[TP+FN]/tot\n",
        "\n",
        "    min_violation = 99\n",
        "    min_label = None\n",
        "\n",
        "    for label, score in res.items():\n",
        "      for label_2, score_2 in res.items():\n",
        "        if label == label_2:\n",
        "          pass\n",
        "        vio = score-score_2\n",
        "        if vio < min_violation:\n",
        "          min_violation = vio\n",
        "          min_label = label\n",
        "    \n",
        "    assert(min_violation != 99)\n",
        "    assert(-1 <= min_violation <= 0)\n",
        "    assert(min_label != None)\n",
        "    return res , min_label, min_violation\n",
        "\n",
        "\n",
        "  # return dict with {metric_name: (score, highest violation label, violation)}\n",
        "  def calculate_all(self):\n",
        "    return_dict = {}\n",
        "    return_dict['Demographic_Parity'] = self.Demographic_Parity()\n",
        "    return_dict['Equal_Opportunity'] = self.Equal_Opportunity()\n",
        "    return_dict['Equal_Odds'] = self.Equal_Odds()\n",
        "    return_dict['Overall_accuracy'] = self.Overall_accuracy()\n",
        "    return return_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjsSi8mJnRjA",
        "outputId": "6ab4e21f-3283-49a1-a23e-6c30e89f4a1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Demographic_Parity': ({0: array([1.]), 1: array([0.33333333])}, 1, array([-0.66666667])), 'Equal_Opportunity': ({0: array([0.5]), 1: array([0.33333333])}, 1, array([-0.16666667])), 'Equal_Odds': ({0: array([1.]), 1: array([0.33333333])}, 1, array([-0.66666667])), 'Overall_accuracy': ({0: array([0.5]), 1: array([0.66666667])}, 0, array([-0.16666667]))}\n"
          ]
        }
      ],
      "source": [
        "protected_label = [0, 1, 0, 1 , 1]\n",
        "predict_label = [1,0,1,1,0]\n",
        "true_label = [1, 1, 0, 1, 0 ]\n",
        "\n",
        "metric = Fairness_metric(protected_label, true_label, predict_label)\n",
        "metric.process()\n",
        "#print(metric.Overall_accuracy())\n",
        "all_data = metric.calculate_all()\n",
        "c_label = all_data['Overall_accuracy'][2]\n",
        "print(all_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes"
      ],
      "metadata": {
        "id": "uWcmrDqf4r_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep unused sample to a pool so that it is easy to set it to biased group\n",
        "\n",
        "For Shekhar, make sure to use the same delta "
      ],
      "metadata": {
        "id": "zAnzvaAH4uGY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2AS6CN95QY89",
        "Nnrbzi6BQczJ",
        "Sv3YZ7UcLlJI",
        "QBM1fsQsnYtl",
        "x6isP1haSRs7",
        "BZSRDLsOOYbw",
        "1qriYYMy8a9v",
        "dJws2nDZSZkw"
      ],
      "name": "Updated main.ipynb",
      "provenance": [],
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}